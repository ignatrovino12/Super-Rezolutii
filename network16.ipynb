{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92851b23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, RandomCrop, GaussianBlur, RandomHorizontalFlip, RandomVerticalFlip, RandomRotation, ColorJitter, ToTensor, RandomApply, RandomPerspective, RandomAffine\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout2d(p=0.3) \n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x) \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        return x + residual\n",
    "\n",
    "class SRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRNN, self).__init__()\n",
    "        self.input_conv = nn.Conv2d(3, 32, kernel_size=9, padding=4)\n",
    "        self.residual_blocks = nn.Sequential(*[ResidualBlock(32) for _ in range(5)])\n",
    "        self.channel_reduction = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "\n",
    "        # 4x upsampling blocks (2x2x2x2)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bicubic', align_corners=False),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_conv = nn.Conv2d(16, 3, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_input = F.interpolate(x, scale_factor=16, mode='bicubic', align_corners=False)\n",
    "        x = self.relu(self.input_conv(x))\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.relu(self.channel_reduction(x))\n",
    "        x = self.upsample(x)\n",
    "        x = self.output_conv(x)\n",
    "        return x + x_input\n",
    "\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, img_path, crop_size=64, scale=16, transform=None):\n",
    "        self.hr_image = Image.open(img_path).convert('RGB')\n",
    "        self.crop_size = crop_size\n",
    "        self.scale = scale\n",
    "        self.transform = transform or Compose([\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomVerticalFlip(),\n",
    "            RandomRotation(degrees=90),\n",
    "            ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.3),\n",
    "            RandomApply([GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.5),\n",
    "            RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "            RandomAffine(degrees=15, translate=(0.05, 0.05), scale=(0.95, 1.05))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return 3000  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        width, height = self.hr_image.size\n",
    "        scaled_crop = self.crop_size * self.scale\n",
    "\n",
    "        left = random.randint(0, width - scaled_crop)\n",
    "        top = random.randint(0, height - scaled_crop)\n",
    "        hr_crop = self.hr_image.crop((left, top, left + scaled_crop, top + scaled_crop))\n",
    "        hr_crop = self.transform(hr_crop)\n",
    "\n",
    "        lr_crop = hr_crop.resize((self.crop_size, self.crop_size), Image.BICUBIC)\n",
    "\n",
    "        return ToTensor()(lr_crop), ToTensor()(hr_crop)\n",
    "\n",
    "def calculate_psnr(output, target, max_pixel_value=1.0):\n",
    "    mse = torch.mean((output - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * math.log10(max_pixel_value ** 2 / mse.item())\n",
    "    return psnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fa922",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SRNN().to(device)\n",
    "# model.load_state_dict(torch.load('srnn_model.pth'))\n",
    "# model.eval()\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "train_dataset = SRDataset('/kaggle/input/datasetscale16v2/1.jpg', crop_size=32, scale=16)\n",
    "val_dataset = SRDataset('/kaggle/input/datasetscale16v2/2.jpg', crop_size=32, scale=16)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "best_psnr = 0.0\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "\n",
    "    for lr, hr in train_loader:\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        sr = model(lr)\n",
    "        loss = criterion(sr, hr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * lr.size(0)\n",
    "        total_psnr += calculate_psnr(sr, hr) * lr.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    avg_psnr = total_psnr / len(train_loader.dataset)\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_psnr = 0.0\n",
    "    with torch.no_grad():\n",
    "        for lr, hr in val_loader:\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr = model(lr)\n",
    "            loss = criterion(sr, hr)\n",
    "            val_loss += loss.item() * lr.size(0)\n",
    "            val_psnr += calculate_psnr(sr, hr) * lr.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_psnr /= len(val_loader.dataset)\n",
    "\n",
    "    if val_psnr > best_psnr:\n",
    "        best_psnr = val_psnr\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_srnn_model.pth')\n",
    "        print(f\"[Epoch {epoch+1}] Model salvat (Val PSNR: {val_psnr:.2f} dB)\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Train PSNR: {avg_psnr:.2f} dB | Val PSNR: {val_psnr:.2f} dB\")\n",
    "\n",
    "torch.save(model.state_dict(), '/kaggle/working/srnn_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
